version: "3.8"

services:
  llama-cpp:
    image: ghcr.io/ggml-org/llama.cpp:full
    container_name: llama-cpp-server
    ports:
      - "8080:8080"
    volumes:
      - ./models:/models:Z
    command:
      - --server
      - --host
      - "0.0.0.0"
      - --port
      - "8080"
      - --model
      - /models/GLM-4.7-Flash-UD-Q8_K_XL.gguf
      - --jinja
      - --ctx-size 
      - 202752
      - --threads
      - -1
      - --fit
      - on
      - --temp
      - "0.2"
      - --top-p
      - "0.95"
      - --min-p
      - "0.01"
      - --flash-attn
      - "on"
      - --sleep-idle-seconds
      - "300"
    restart: unless-stopped

